// src/api/integrations.js - MIMITECH Backend Integration

/**
 * Upload File
 */
export async function uploadFile(file) {
  const apiBase = import.meta.env.VITE_API_BASE || "http://localhost:8000";
  
  const fd = new FormData();
  fd.append("file", file);
  const res = await fetch(`${apiBase}/api/upload`, {
    method: "POST",
    body: fd,
    credentials: "include",
  });
  if (!res.ok) {
    const error = await res.json().catch(() => ({ message: "Upload failed" }));
    throw new Error(error.message || "Upload failed");
  }
  return res.json();
}

/**
 * Analyze Abrechnung
 */
export async function analyzeAbrechnung(abrechnungId) {
  const apiBase = import.meta.env.VITE_API_BASE || "http://localhost:8000";

  const res = await fetch(`${apiBase}/api/analyze`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ abrechnung_id: abrechnungId }),
    credentials: "include",
  });
  if (!res.ok) throw new Error("Analyse failed");
  return res.json();
}

/**
 * Get Report
 */
export async function getReport(abrechnungId) {
  const apiBase = import.meta.env.VITE_API_BASE || "http://localhost:8000";

  const res = await fetch(`${apiBase}/api/report/${abrechnungId}`, {
    credentials: "include",
  });
  if (!res.ok) throw new Error("Report failed");
  return res.json();
}

// Legacy-Exports für Kompatibilität
export const Core = {};

/**
 * InvokeLLM - OpenAI Integration über Backend
 */
export const InvokeLLM = async ({ prompt, system_prompt, temperature, max_tokens }) => {
  const apiBase = import.meta.env.VITE_API_BASE || "http://localhost:8000";
  
  try {
    const res = await fetch(`${apiBase}/api/llm/invoke`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        prompt,
        system_prompt,
        temperature: temperature || 0.7,
        max_tokens: max_tokens || 4096
      }),
      credentials: "include"
    });
    
    if (!res.ok) {
      const error = await res.json().catch(() => ({ detail: "LLM-Anfrage fehlgeschlagen" }));
      throw new Error(error.detail || "LLM service unavailable");
    }
    
    return res.json();
  } catch (error) {
    console.error('[LLM Error]', error);
    return {
      content: `⚠️ **Backend nicht erreichbar**\n\nDie KI-Funktion ist derzeit nicht verfügbar.\n\n**Error:** ${error.message}`,
      usage: { total_tokens: 0 },
      model: "offline",
      finish_reason: "error"
    };
  }
};

/**
 * ExtractDataFromUploadedFile - PDF-Extraktion
 */
export const ExtractDataFromUploadedFile = async (fileId) => {
  const apiBase = import.meta.env.VITE_API_BASE || "http://localhost:8000";
  
  try {
    const res = await fetch(`${apiBase}/api/extract-data/${fileId}`, {
      method: "POST",
      credentials: "include"
    });
    
    if (!res.ok) throw new Error("PDF extraction failed");
    
    return res.json();
  } catch (error) {
    console.error('[PDF Extraction Error]', error);
    return {
      success: false,
      data: {
        titel: "Extraktion fehlgeschlagen",
        abrechnungszeitraum: new Date().getFullYear().toString(),
        verwalter: "Nicht erkannt",
        objekt_adresse: "Nicht erkannt",
        gesamtkosten: 0,
        positionen: []
      }
    };
  }
};

export const SendEmail = async () => ({ success: true });
export const UploadFile = uploadFile;
export const GenerateImage = async () => ({ url: "https://via.placeholder.com/400" });
export const CreateFileSignedUrl = async () => ({ url: "#" });
export const UploadPrivateFile = uploadFile;
